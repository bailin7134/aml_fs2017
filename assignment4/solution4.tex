% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{color}
\usepackage{tikz, pgfplots}
\usepackage{graphicx}
\usepackage{epstopdf} %converting to PDF
\usepackage{subcaption}
\usepackage{listings}

\makeatletter

\renewcommand\section{\@startsection {section}{1}{\z@}%
	{-3.5ex \@plus -1ex \@minus -.2ex}%
	{2.3ex \@plus.2ex}%
	{\normalfont\large\bfseries}}% from \Large
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
	{-3.25ex\@plus -1ex \@minus -.2ex}%
	{1.5ex \@plus .2ex}%
	{\normalfont\large\bfseries}}% from \large
\makeatother

\begin{document}
	
	% --------------------------------------------------------------
	%                         Start here
	% --------------------------------------------------------------
	
	%\renewcommand{\qedsymbol}{\filledbox}
	
	\title{\textbf{Advanced Topics in Machine Learning Exercise \#{4}}\\
	Universit{\"a}t Bern}%replace X with the appropriate number
	\author{{Lin Bai, 09935404}} %replace with your name
	
	\maketitle

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%   question 1
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Solution to question 1}
	\textbf{What are the main motivations of using Convolutional Neural Networks (CNN)?}\\
	Convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex.\\
	\\
	Advantages:\\
	The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units.\\

	%\begin{figure}[htpb]
	%	\centering
	%	\includegraphics[width=0.7\textwidth]{ReLU.png}
	%	\caption{ReLU function}
	%\end{figure}
	\noindent
	\textbf{Why do we need regularization in machine learning? Give two examples of regularization in deep learning.}\\
	Since in most machine learning problems, we do not have the required number of training samples or the model complexity is large we have to use regularization in order to avoid, or lessen the possibility, of over-fitting.\\
	\\
	examples:\\
	1. Dataset augmentation\\
	2. Early stopping\\
	3. Dropout layer\\
	4. Weight penalty L1 and L2\\
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%   question 2
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Solution to question 2}
	The code is listed below.
	\lstset{language=[5.0]Lua}
	\lstset{frame=lines}
	\lstset{basicstyle=\footnotesize\ttfamily}
	\begin{lstlisting}[breaklines=true]
	require 'nngraph'
	require 'torch'
	require 'nn'   
	require 'optim'
	
	-- define labels
	local label1 = torch.rand(1,3,64,64)
	local label2 = torch.rand(1,3,64,64)
	-- TODO
	local function createResBlock()
		-- local resBlock = nn.Sequential()
		-- TODO implement resblock in this function
		-- do not implement this separately
		local cat = nn.Sequential()
		cat:add(nn.SpatialConvolution(32, 32, 3, 3,1,1,1,1))
		cat:add(nn.ReLU(true))
		cat:add(nn.SpatialConvolution(32, 32, 3, 3,1,1,1,1))
		-- concate block
		local conBlock = nn.ConcatTable()
		conBlock:add(nn.Identity())
		conBlock:add(cat)
		--local conResult = conBlock:forward(input)
		-- add block
		--local addBlock = nn.CAddTable()({conBlock})
		local resBlock = nn.Sequential()
		resBlock:add(conBlock)
		resBlock:add(nn.CAddTable())
		if arg[1] == "debug" then
			print(resBlock)
		end
		return resBlock
	end
	local model = nn.ParallelTable()
	local L1Net = nn.Sequential()
	local L2Net = nn.Sequential()
	local conv1 = nn.Sequential()
	local conv4 = nn.Sequential()
	-- Define conv1 and conv4 layers
	conv1:add(nn.SpatialConvolution(3, 32, 3, 3,1,1,1,1))
	conv4:add(nn.SpatialConvolution(32, 3, 3, 3,1,1,1,1))
	
	-- TODO add shared conv1 layer to L1Net and L2Net
	L1Net:add(conv1)
	L2Net = L1Net:clone('weight', 'bias', 'gradWeight', 'gradBias')
	-- TODO add ResBlock to L1Net and L2Net
	L1Net:add(createResBlock())
	L2Net:add(createResBlock())
	-- TODO add shared conv4 layer to L1Net and L2Net
	L1Net:add(conv4)
	L2Net = L1Net:clone('weight', 'bias', 'gradWeight', 'gradBias')
	-- TODO add L1Net and L2Net to model
	model:add(L1Net)
	model:add(L2Net)
	if arg[1] == "debug" then
	print(model)
	end
	
	-- define criterion
	criterion1 = nn.MSECriterion()
	criterion2 = nn.AbsCriterion()
	if model then
	parameters,gradParameters = model:getParameters()
	end
	print '==> configuring optimizer'
	optimMethod = optim.adam
	local parameters, gradParameters = model:getParameters()
	feval = function(x)
		model:zeroGradParameters()
		local inputs1 = torch.rand(1,3,64,64)
		local inputs2 = torch.rand(1,3,64,64)
		outputs = model:forward({inputs1, inputs2}) 
		err1 = criterion1:forward(outputs[1], label1)
		err2 = criterion2:forward(outputs[2], label2)
	
		local gradOutputs1 = criterion1:backward(outputs[1], label1)
		local gradOutputs2 = criterion2:backward(outputs[2], label2)
	
		model:backward({inputs1, inputs2}, {gradOutputs1, gradOutputs2})
		err = err1 + err2
		return err, gradParameters
	end
	-- run 10 iterations
	for iii=1,10 do
		optim.adam(feval, parameters, optimState)
		print(err)
	end
	-- Test whether parameters are shared, don't remove this part
	local m1 = nn.Sequential()
	m1:add(model.modules[1].modules[1])
	local m2 = nn.Sequential()
	m2:add(model.modules[2].modules[1])
	parameters1,gradParameters1 = m1:getParameters()
	parameters2,gradParameters2 = m2:getParameters()
	local diff = parameters1 - parameters2
	print(diff:min())
	print(diff:max())
	\end{lstlisting}
	And the output is
	\begin{lstlisting}[breaklines=true]
	==> configuring optimizer
	1.0675603646911	
	0.74996335809637	
	0.49105424236534	
	0.36992482230006	
	0.39573809374814	
	0.36977156441981	
	0.4221687095061	
	0.37449108905438	
	0.4512566722061	
	0.37269264931418	
	0	
	0
	\end{lstlisting}		
	% --------------------------------------------------------------
	%     You don't have to mess with anything below this line.
	% --------------------------------------------------------------
	
\end{document}